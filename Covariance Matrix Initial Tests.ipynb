{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c59599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sf_quant as sf\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0703495",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.date(1996, 1, 1)\n",
    "end = dt.date(2024, 12, 31)\n",
    "\n",
    "columns = [\n",
    "    'date',\n",
    "    'barrid',\n",
    "    'price',\n",
    "    'return',\n",
    "    'specific_return',\n",
    "]\n",
    "\n",
    "df = pl.read_parquet(\"russell_3000_daily.parquet\")\n",
    "\n",
    "df.write_parquet(\"russell_3000_daily.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e257f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USAR6S1\n",
      "shape: (743, 2)\n",
      "┌────────────┬─────────┐\n",
      "│ date       ┆ return  │\n",
      "│ ---        ┆ ---     │\n",
      "│ date       ┆ f64     │\n",
      "╞════════════╪═════════╡\n",
      "│ 2013-01-02 ┆ 3.0365  │\n",
      "│ 2013-01-03 ┆ 2.7073  │\n",
      "│ 2013-01-04 ┆ 1.7298  │\n",
      "│ 2013-01-07 ┆ -0.2429 │\n",
      "│ 2013-01-08 ┆ -0.4464 │\n",
      "│ …          ┆ …       │\n",
      "│ 2015-12-07 ┆ -0.0266 │\n",
      "│ 2015-12-08 ┆ 0.0266  │\n",
      "│ 2015-12-09 ┆ 0.0     │\n",
      "│ 2015-12-10 ┆ 0.0     │\n",
      "│ 2015-12-11 ┆ 0.0     │\n",
      "└────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "#Helper program to find stocks that have returns over the whole range of dates. I need to simulate the signals somehow.\n",
    "\n",
    "startDate = dt.date(2013, 1, 1) \n",
    "endDate = dt.date(2018, 1, 1)\n",
    "\n",
    "unique_barrids = df.filter(\n",
    "    pl.col(\"date\").is_between(startDate, endDate)\n",
    ").select(pl.col(\"barrid\")).unique()\n",
    "\n",
    "barrid = unique_barrids.sample(1)[0, 0]\n",
    "print(barrid)\n",
    "\n",
    "print(df.filter(\n",
    "    pl.col(\"date\").is_between(startDate, endDate),\n",
    "    (pl.col(\"barrid\") == barrid),\n",
    ").select(\n",
    "    pl.col(\"date\").alias(\"date\"),\n",
    "    pl.col(\"return\").alias(\"return\")\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce1b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_259, 4)\n",
      "┌────────────┬───────────┬───────────┬───────────┐\n",
      "│ date       ┆ returns1  ┆ returns2  ┆ returns3  │\n",
      "│ ---        ┆ ---       ┆ ---       ┆ ---       │\n",
      "│ date       ┆ f64       ┆ f64       ┆ f64       │\n",
      "╞════════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 2013-01-02 ┆ 0.005376  ┆ 0.022719  ┆ 0.014524  │\n",
      "│ 2013-01-03 ┆ 0.032086  ┆ -0.008599 ┆ -0.003937 │\n",
      "│ 2013-01-04 ┆ 0.036269  ┆ 0.010481  ┆ -0.017248 │\n",
      "│ 2013-01-07 ┆ -0.0002   ┆ 0.010372  ┆ 0.001097  │\n",
      "│ 2013-01-08 ┆ -0.002801 ┆ 0.007788  ┆ 0.011687  │\n",
      "│ …          ┆ …         ┆ …         ┆ …         │\n",
      "│ 2017-12-22 ┆ -0.033333 ┆ 0.000254  ┆ 0.007785  │\n",
      "│ 2017-12-26 ┆ 0.0       ┆ 0.002284  ┆ 0.002107  │\n",
      "│ 2017-12-27 ┆ 0.0       ┆ -0.000506 ┆ -0.001402 │\n",
      "│ 2017-12-28 ┆ -0.002463 ┆ -0.003799 ┆ 0.009123  │\n",
      "│ 2017-12-29 ┆ -0.001235 ┆ -0.005594 ┆ 0.004172  │\n",
      "└────────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "barrid1 = \"USAQ392\"\n",
    "barrid2 = \"USAZ6Q1\"\n",
    "barrid3 = \"USAROU1\"\n",
    "\n",
    "\n",
    "training_data = df.filter(\n",
    "    pl.col(\"date\").is_between(startDate, endDate),\n",
    "    (pl.col(\"barrid\") == barrid1) | (pl.col(\"barrid\") == barrid2) | (pl.col(\"barrid\") == barrid3)\n",
    ").pivot(\"barrid\", index=\"date\", values=\"return\").select(\n",
    "    pl.col(\"date\"),\n",
    "    (pl.col(barrid1)/100).alias(\"returns1\"),\n",
    "    (pl.col(barrid2)/100).alias(\"returns2\"),\n",
    "    (pl.col(barrid3)/100).alias(\"returns3\")\n",
    ")\n",
    "\n",
    "print(training_data)\n",
    "\n",
    "#cum_returns = trainingdata.select(pl.col(\"date\"), np.log(pl.col(\"return\") + 1).cum_sum().alias(\"cumulative_returns\"))\n",
    "\n",
    "#print(cum_returns.select(pl.col(\"date\"), (np.exp(pl.col(\"cumulative_returns\"))-1).alias(\"Cumulative_Returns\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d01455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_200, 26)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ date      ┆ returns1  ┆ returns2  ┆ returns3  ┆ … ┆ est_cov31 ┆ est_cov32 ┆ est_cov33 ┆ cov_erro │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ r        │\n",
      "│ date      ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ ---      │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 2013-03-2 ┆ 0.023904  ┆ 0.008825  ┆ 0.025592  ┆ … ┆ 0.000037  ┆ 0.000024  ┆ 0.00016   ┆ 8.3230e- │\n",
      "│ 8         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
      "│ 2013-04-0 ┆ -0.0508   ┆ -0.023443 ┆ -0.022719 ┆ … ┆ 0.000042  ┆ 0.000028  ┆ 0.000177  ┆ 0.000002 │\n",
      "│ 1         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2013-04-0 ┆ -0.003416 ┆ -0.031888 ┆ -0.001524 ┆ … ┆ 0.000063  ┆ 0.00003   ┆ 0.000191  ┆ 0.000003 │\n",
      "│ 2         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2013-04-0 ┆ 0.002285  ┆ -0.023316 ┆ -0.055725 ┆ … ┆ 0.000014  ┆ -0.000015 ┆ 0.000198  ┆ 0.000002 │\n",
      "│ 3         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2013-04-0 ┆ 0.006612  ┆ 0.009094  ┆ -0.001213 ┆ … ┆ -0.000045 ┆ -0.000021 ┆ 0.000213  ┆ 0.000012 │\n",
      "│ 4         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
      "│ 2017-12-2 ┆ -0.033333 ┆ 0.000254  ┆ 0.007785  ┆ … ┆ 0.00019   ┆ 0.000038  ┆ 0.000569  ┆ 9.4357e- │\n",
      "│ 2         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
      "│ 2017-12-2 ┆ 0.0       ┆ 0.002284  ┆ 0.002107  ┆ … ┆ 0.000214  ┆ 0.00004   ┆ 0.000631  ┆ 9.1955e- │\n",
      "│ 6         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
      "│ 2017-12-2 ┆ 0.0       ┆ -0.000506 ┆ -0.001402 ┆ … ┆ 0.000235  ┆ 0.000041  ┆ 0.000701  ┆ 0.000001 │\n",
      "│ 7         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2017-12-2 ┆ -0.002463 ┆ -0.003799 ┆ 0.009123  ┆ … ┆ 0.000253  ┆ 0.000039  ┆ 0.000774  ┆ 0.000001 │\n",
      "│ 8         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2017-12-2 ┆ -0.001235 ┆ -0.005594 ┆ 0.004172  ┆ … ┆ 0.000292  ┆ 0.000043  ┆ 0.000829  ┆ 0.000001 │\n",
      "│ 9         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "0.011194405097652708\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = 0.9 #ratio for geometric weighting.\n",
    "b = 0.9\n",
    "\n",
    "error = 0.01 #approximately the contribution of the last day included in the average.\n",
    "num_r = int(np.floor_divide(np.log(error), np.log(r))) #number of days computed in the average.\n",
    "weights_r = [(r**i) for i in range(1, num_r + 1)] #weights for the rolling sum. Note they are automatially normalized.\n",
    "\n",
    "num_b = int(np.floor_divide(np.log(error), np.log(b))) #number of days computed in the average.\n",
    "weights_b = [(b**i) for i in range(1, num_r + 1)] #weights for the rolling sum. Note they are automatially normalized.\n",
    "\n",
    "exprs_cov_construction = [pl.col(\"date\").alias(\"date\")] #Expressions for constructing columns representing covariance matrix coefficients.\n",
    "for i in range(1, 4):\n",
    "    for j in range(1, 4):\n",
    "        exprs_cov_construction.append(\n",
    "            ((pl.col(f\"returns{i}\") - pl.col(f\"mu{i}\")) *\n",
    "            (pl.col(f\"returns{j}\") - pl.col(f\"mu{j}\"))).alias(f\"cov{i}{j}\")\n",
    "        )\n",
    "\n",
    "exprs_cov_est = [pl.col(\"date\").alias(\"date\")] #Expressions for constructing estimated covariance matrix coefficients.\n",
    "for i in range(1, 4):\n",
    "    for j in range(1, 4):\n",
    "        exprs_cov_est.append(\n",
    "            pl.col(f\"cov{i}{j}\").rolling_mean(num_r, weights_r).fill_null(strategy=\"backward\").alias(f\"est_cov{i}{j}\")\n",
    "        )\n",
    "\n",
    "cov_data = training_data.with_columns(\n",
    "    pl.col(\"returns1\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu1\"),\n",
    "    pl.col(\"returns2\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu2\"),\n",
    "    pl.col(\"returns3\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu3\")\n",
    ").with_columns( #Need the mu's to compute the covariance matrices.\n",
    "    exprs_cov_construction\n",
    ").with_columns( #Need the covariance matrix coefficients to compute the estimated future covariance matrix.\n",
    "    exprs_cov_est\n",
    ").with_columns(\n",
    "    sum(np.square(pl.col(f\"cov{i}{j}\").shift(1) - pl.col(f\"est_cov{i}{j}\")) for i in range(1, 4) for j in range(1, 4)).alias(\"cov_error\")\n",
    ").filter(\n",
    "    pl.col(\"date\").is_between(startDate + dt.timedelta(num_r + num_b), endDate) \n",
    "    #rolling_mean introduces a null in a row when there are too many weights_r for the num_rber of available elements. \n",
    "    #this is just filtering out backfilled nulls from two rolling_mean steps.\n",
    ")\n",
    "#pl.col(f\"est_cov{i}{j}\") is the estimated future covariance matrix at time step t. We need to compare it to the actual covariance matrix at time t+1.\n",
    "\n",
    "print(cov_data)\n",
    "\n",
    "#We now compute the element-wise squared error of the estimated covariance matrix for each t.\n",
    "error = cov_data.select(pl.sum(\"cov_error\"))[0, 0]\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd27fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cov_error(r, b): #ratio for geometric weighting.\n",
    "    error = 0.01 #approximately the contribution of the last day included in the average.\n",
    "    \n",
    "    num_r = int(np.floor_divide(np.log(error), np.log(r))) #num_rber of days computed in the average.\n",
    "    weights_r = [(r**i) for i in range(1, num_r + 1)] #weights_r for the rolling sum. Note they are automatially normalized.\n",
    "    \n",
    "    num_b = int(np.floor_divide(np.log(error), np.log(b))) #num_rber of days computed in the average.\n",
    "    weights_b = [(b**i) for i in range(1, num_b + 1)] #weights_r for the rolling sum. Note they are automatially normalized.\n",
    "\n",
    "    exprs_cov_construction = [pl.col(\"date\").alias(\"date\")] #Expressions for constructing columns representing covariance matrix coefficients.\n",
    "    for i in range(1, 4):\n",
    "        for j in range(1, 4):\n",
    "            exprs_cov_construction.append(\n",
    "                ((pl.col(f\"returns{i}\") - pl.col(f\"mu{i}\")) *\n",
    "                (pl.col(f\"returns{j}\") - pl.col(f\"mu{j}\"))).alias(f\"cov{i}{j}\")\n",
    "            )\n",
    "\n",
    "    exprs_cov_est = [pl.col(\"date\").alias(\"date\")] #Expressions for constructing estimated covariance matrix coefficients.\n",
    "    for i in range(1, 4):\n",
    "        for j in range(1, 4):\n",
    "            exprs_cov_est.append(\n",
    "                pl.col(f\"cov{i}{j}\").rolling_mean(num_r, weights_r).fill_null(strategy=\"backward\").alias(f\"est_cov{i}{j}\")\n",
    "            )\n",
    "\n",
    "    cov_data = training_data.with_columns(\n",
    "        pl.col(\"returns1\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu1\"),\n",
    "        pl.col(\"returns2\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu2\"),\n",
    "        pl.col(\"returns3\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu3\")\n",
    "    ).with_columns( #Need the mu's to compute the covariance matrices.\n",
    "        exprs_cov_construction\n",
    "    ).with_columns( #Need the covariance matrix coefficients to compute the estimated future covariance matrix.\n",
    "        exprs_cov_est\n",
    "    ).with_columns(\n",
    "        np.sqrt(sum(np.square(pl.col(f\"cov{i}{j}\").shift(-1) - pl.col(f\"est_cov{i}{j}\")) for i in range(1, 4) for j in range(1, 4))).alias(\"cov_error\"), #Error in the estimated covariance matrix \n",
    "        np.sqrt(sum(np.square(pl.col(f\"cov{i}{i}\").shift(-1) - pl.col(f\"est_cov{i}{i}\")) for i in range(1, 3))).alias(\"cov_error_null\"), #This is to test that we actually get an improved error estimate when we consider covariances (not just variances)\n",
    "        np.sqrt(sum(np.square(pl.col(f\"cov{i}{j}\")) for i in range(1, 4) for j in range(1, 4))).alias(\"cov_mag\")\n",
    "    ).with_columns(\n",
    "        np.divide(pl.col(\"cov_error\"), pl.col(\"cov_mag\")).alias(\"cov_rel_error\")\n",
    "    ).filter(\n",
    "        pl.col(\"date\").is_between(startDate + dt.timedelta(num_r + num_b), endDate)\n",
    "        #rolling_mean introduces a null in a row when there are too many weights_r for the num_rber of available elements. \n",
    "        #this is just filtering out backfilled nulls from two rolling_mean steps.\n",
    "    )\n",
    "    #pl.col(f\"est_cov{i}{j}\") is the estimated future covariance matrix at time step t. We need to compare it to the actual covariance matrix at time t+1.\n",
    "\n",
    "    #print(cov_data)\n",
    "\n",
    "    #We now compute the element-wise squared error of the estimated covariance matrix for each t.\n",
    "    cov_error = cov_data.select(pl.mean(\"cov_error\"))[0, 0]\n",
    "    cov_mag = cov_data.select(pl.mean(\"cov_mag\"))[0, 0]\n",
    "    cov_error_null = cov_data.select(pl.mean(\"cov_error_null\"))[0, 0]\n",
    "\n",
    "    #cov_rel_error = cov_data.select(pl.mean(\"cov_rel_error\"))[0, 0]  #I think volatility makes this a bad measure\n",
    "\n",
    "    return cov_error, cov_mag, cov_error_null, [r, b], (num_r + num_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11433422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1, Cov Error: 0.0017122572149002948, Cov Mag: 0.0011935513072305309\n",
      "i = 2, Cov Error: 0.0016705854516756802, Cov Mag: 0.0011935513072305309\n",
      "i = 3, Cov Error: 0.0016213574310335777, Cov Mag: 0.0011915176543595778\n",
      "i = 4, Cov Error: 0.0015654773420463704, Cov Mag: 0.0011915176543595778\n",
      "i = 5, Cov Error: 0.0015326943824483094, Cov Mag: 0.0011915176543595778\n",
      "i = 6, Cov Error: 0.001474680848593366, Cov Mag: 0.0011884507808995657\n",
      "i = 7, Cov Error: 0.001451157849326301, Cov Mag: 0.0011889302898977175\n",
      "i = 8, Cov Error: 0.001410961202641907, Cov Mag: 0.0011864094635682322\n",
      "i = 9, Cov Error: 0.0013218169479247222, Cov Mag: 0.0011900596504356609\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    print(f\"i = {i}, Cov Error: {cov_error(i/10.0, 0.9)[0]}, Cov Mag: {cov_error(i/10.0, 0.9)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = 950\n",
    "L = 900\n",
    "\n",
    "M = 1000.0\n",
    "Len = (U-L)\n",
    "Cov_Errors = [[cov_error(i/M, j/M) for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feacbe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1083529920168278\n",
      "(0.0012936687827024267, 0.0011785312830885496, 0.0008603427786633642, [0.9164, 0.9153], 104)\n"
     ]
    }
   ],
   "source": [
    "training_days = (startDate - endDate).days\n",
    "\n",
    "Sample_Size = [[Cov_Errors[i-L][j-L][4] for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "Abs_Errors = [[Cov_Errors[i-L][j-L][0]*(1+np.divide(1, Sample_Size[i-L][j-L]-1)) for i in range(L, U)] for j in range(L, U)]\n",
    "Cov_Mag = [[Cov_Errors[i-L][j-L][1] for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "Rel_Errors = [[np.divide(Abs_Errors[i-L][j-L], Cov_Mag[i-L][j-L]) for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "min = np.argmin(Rel_Errors)\n",
    "\n",
    "print(Rel_Errors[min//Len][min%Len])\n",
    "print(Cov_Errors[min//Len][min%Len])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547512b",
   "metadata": {},
   "source": [
    "0.958157121980979\n",
    "(8.415090972179432e-06, 8.86868308873483e-06, [0.9152, 0.9153], 103)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orthogonal-alphas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
