{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c59599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/QuantBattle2025/orthogonal-alphas/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-26 21:29:22,171\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import sf_quant as sf\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0703495",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): russell_3000_daily.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m end = dt.date(\u001b[32m2024\u001b[39m, \u001b[32m12\u001b[39m, \u001b[32m31\u001b[39m)\n\u001b[32m      4\u001b[39m columns = [\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbarrid\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mspecific_return\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrussell_3000_daily.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m df.write_parquet(\u001b[33m\"\u001b[39m\u001b[33mrussell_3000_daily.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/QuantBattle2025/orthogonal-alphas/.venv/lib/python3.13/site-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/QuantBattle2025/orthogonal-alphas/.venv/lib/python3.13/site-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/QuantBattle2025/orthogonal-alphas/.venv/lib/python3.13/site-packages/polars/io/parquet/functions.py:289\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, missing_columns, allow_missing_columns)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    287\u001b[39m         lf = lf.select(columns)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/QuantBattle2025/orthogonal-alphas/.venv/lib/python3.13/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/QuantBattle2025/orthogonal-alphas/.venv/lib/python3.13/site-packages/polars/lazyframe/opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/QuantBattle2025/orthogonal-alphas/.venv/lib/python3.13/site-packages/polars/lazyframe/frame.py:2407\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2405\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2406\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory (os error 2): russell_3000_daily.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "start = dt.date(1996, 1, 1)\n",
    "end = dt.date(2024, 12, 31)\n",
    "\n",
    "columns = [\n",
    "    'date',\n",
    "    'barrid',\n",
    "    'price',\n",
    "    'return',\n",
    "    'specific_return',\n",
    "]\n",
    "\n",
    "df = pl.read_parquet(\"russell_3000_daily.parquet\")\n",
    "\n",
    "df.write_parquet(\"russell_3000_daily.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USAR6S1\n",
      "shape: (743, 2)\n",
      "┌────────────┬─────────┐\n",
      "│ date       ┆ return  │\n",
      "│ ---        ┆ ---     │\n",
      "│ date       ┆ f64     │\n",
      "╞════════════╪═════════╡\n",
      "│ 2013-01-02 ┆ 3.0365  │\n",
      "│ 2013-01-03 ┆ 2.7073  │\n",
      "│ 2013-01-04 ┆ 1.7298  │\n",
      "│ 2013-01-07 ┆ -0.2429 │\n",
      "│ 2013-01-08 ┆ -0.4464 │\n",
      "│ …          ┆ …       │\n",
      "│ 2015-12-07 ┆ -0.0266 │\n",
      "│ 2015-12-08 ┆ 0.0266  │\n",
      "│ 2015-12-09 ┆ 0.0     │\n",
      "│ 2015-12-10 ┆ 0.0     │\n",
      "│ 2015-12-11 ┆ 0.0     │\n",
      "└────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "#Helper program to find stocks that have returns over the whole range of dates. I need to simulate the signals somehow.\n",
    "\n",
    "startDate = dt.date(2013, 1, 1) \n",
    "endDate = dt.date(2018, 1, 1)\n",
    "\n",
    "unique_barrids = df.filter(\n",
    "    pl.col(\"date\").is_between(startDate, endDate)\n",
    ").select(pl.col(\"barrid\")).unique()\n",
    "\n",
    "barrid = unique_barrids.sample(1)[0, 0]\n",
    "print(barrid)\n",
    "\n",
    "print(df.filter(\n",
    "    pl.col(\"date\").is_between(startDate, endDate),\n",
    "    (pl.col(\"barrid\") == barrid),\n",
    ").select(\n",
    "    pl.col(\"date\").alias(\"date\"),\n",
    "    pl.col(\"return\").alias(\"return\")\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_259, 4)\n",
      "┌────────────┬───────────┬───────────┬───────────┐\n",
      "│ date       ┆ returns1  ┆ returns2  ┆ returns3  │\n",
      "│ ---        ┆ ---       ┆ ---       ┆ ---       │\n",
      "│ date       ┆ f64       ┆ f64       ┆ f64       │\n",
      "╞════════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 2013-01-02 ┆ 0.005376  ┆ 0.022719  ┆ 0.014524  │\n",
      "│ 2013-01-03 ┆ 0.032086  ┆ -0.008599 ┆ -0.003937 │\n",
      "│ 2013-01-04 ┆ 0.036269  ┆ 0.010481  ┆ -0.017248 │\n",
      "│ 2013-01-07 ┆ -0.0002   ┆ 0.010372  ┆ 0.001097  │\n",
      "│ 2013-01-08 ┆ -0.002801 ┆ 0.007788  ┆ 0.011687  │\n",
      "│ …          ┆ …         ┆ …         ┆ …         │\n",
      "│ 2017-12-22 ┆ -0.033333 ┆ 0.000254  ┆ 0.007785  │\n",
      "│ 2017-12-26 ┆ 0.0       ┆ 0.002284  ┆ 0.002107  │\n",
      "│ 2017-12-27 ┆ 0.0       ┆ -0.000506 ┆ -0.001402 │\n",
      "│ 2017-12-28 ┆ -0.002463 ┆ -0.003799 ┆ 0.009123  │\n",
      "│ 2017-12-29 ┆ -0.001235 ┆ -0.005594 ┆ 0.004172  │\n",
      "└────────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "barrid1 = \"USAQ392\"\n",
    "barrid2 = \"USAZ6Q1\"\n",
    "barrid3 = \"USAROU1\"\n",
    "\n",
    "\n",
    "training_data = df.filter(\n",
    "    pl.col(\"date\").is_between(startDate, endDate),\n",
    "    (pl.col(\"barrid\") == barrid1) | (pl.col(\"barrid\") == barrid2) | (pl.col(\"barrid\") == barrid3)\n",
    ").pivot(\"barrid\", index=\"date\", values=\"return\").select(\n",
    "    pl.col(\"date\"),\n",
    "    (pl.col(barrid1)/100).alias(\"returns1\"),\n",
    "    (pl.col(barrid2)/100).alias(\"returns2\"),\n",
    "    (pl.col(barrid3)/100).alias(\"returns3\")\n",
    ")\n",
    "\n",
    "print(training_data)\n",
    "\n",
    "#cum_returns = trainingdata.select(pl.col(\"date\"), np.log(pl.col(\"return\") + 1).cum_sum().alias(\"cumulative_returns\"))\n",
    "\n",
    "#print(cum_returns.select(pl.col(\"date\"), (np.exp(pl.col(\"cumulative_returns\"))-1).alias(\"Cumulative_Returns\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cov_error(r, b): #ratio for geometric weighting.\n",
    "    error = 0.01 #approximately the contribution of the last day included in the average.\n",
    "    \n",
    "    num_r = int(np.floor_divide(np.log(error), np.log(r))) #num_rber of days computed in the average.\n",
    "    weights_r = [(r**i) for i in range(1, num_r + 1)] #weights_r for the rolling sum. Note they are automatially normalized.\n",
    "    \n",
    "    num_b = int(np.floor_divide(np.log(error), np.log(b))) #num_rber of days computed in the average.\n",
    "    weights_b = [(b**i) for i in range(1, num_b + 1)] #weights_r for the rolling sum. Note they are automatially normalized.\n",
    "\n",
    "    exprs_cov_construction = [pl.col(\"date\").alias(\"date\")] #Expressions for constructing columns representing covariance matrix coefficients.\n",
    "    for i in range(1, 4):\n",
    "        for j in range(1, 4):\n",
    "            exprs_cov_construction.append(\n",
    "                ((pl.col(f\"returns{i}\") - pl.col(f\"mu{i}\")) *\n",
    "                (pl.col(f\"returns{j}\") - pl.col(f\"mu{j}\"))).alias(f\"cov{i}{j}\")\n",
    "            )\n",
    "            \n",
    "    exprs_cov_est = [pl.col(\"date\").alias(\"date\")] #Expressions for constructing estimated covariance matrix coefficients.\n",
    "    for i in range(1, 4):\n",
    "        for j in range(1, 4):\n",
    "            exprs_cov_est.append(\n",
    "                pl.col(f\"cov{i}{j}\").rolling_mean(num_r, weights_r).fill_null(strategy=\"backward\").alias(f\"est_cov{i}{j}\")\n",
    "            )\n",
    "\n",
    "    cov_data = training_data.with_columns(\n",
    "        pl.col(\"returns1\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu1\"),\n",
    "        pl.col(\"returns2\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu2\"),\n",
    "        pl.col(\"returns3\").rolling_mean(num_b, weights_b).fill_null(strategy=\"backward\").alias(\"mu3\")\n",
    "    ).with_columns( #Need the mu's to compute the covariance matrices.\n",
    "        exprs_cov_construction\n",
    "    ).with_columns( #Need the covariance matrix coefficients to compute the estimated future covariance matrix.\n",
    "        exprs_cov_est\n",
    "    ).with_columns(\n",
    "        np.sqrt(sum(np.square(pl.col(f\"cov{i}{j}\").shift(-1) - pl.col(f\"est_cov{i}{j}\")) for i in range(1, 4) for j in range(1, 4))).alias(\"cov_error\"), #Error in the estimated covariance matrix \n",
    "        np.sqrt(sum(np.square(pl.col(f\"cov{i}{i}\").shift(-1) - pl.col(f\"est_cov{i}{i}\")) for i in range(1, 3))).alias(\"cov_error_null\"), #This is to test that we actually get an improved error estimate when we consider covariances (not just variances)\n",
    "        np.sqrt(sum(np.square(pl.col(f\"cov{i}{j}\")) for i in range(1, 4) for j in range(1, 4))).alias(\"cov_mag\")\n",
    "    ).with_columns(\n",
    "        np.divide(pl.col(\"cov_error\"), pl.col(\"cov_mag\")).alias(\"cov_rel_error\")\n",
    "    ).filter(\n",
    "        pl.col(\"date\").is_between(startDate + dt.timedelta(num_r + num_b), endDate)\n",
    "        #rolling_mean introduces a null in a row when there are too many weights_r for the num_rber of available elements. \n",
    "        #this is just filtering out backfilled nulls from two rolling_mean steps.\n",
    "    )\n",
    "    #pl.col(f\"est_cov{i}{j}\") is the estimated future covariance matrix at time step t. We need to compare it to the actual covariance matrix at time t+1.\n",
    "\n",
    "    #print(cov_data)\n",
    "\n",
    "    #We now compute the element-wise squared error of the estimated covariance matrix for each t.\n",
    "    cov_error = cov_data.select(pl.mean(\"cov_error\"))[0, 0]\n",
    "    cov_mag = cov_data.select(pl.mean(\"cov_mag\"))[0, 0]\n",
    "    cov_error_null = cov_data.select(pl.mean(\"cov_error_null\"))[0, 0]\n",
    "\n",
    "    #cov_rel_error = cov_data.select(pl.mean(\"cov_rel_error\"))[0, 0]  #I think volatility makes this a bad measure\n",
    "\n",
    "    return cov_error, cov_mag, cov_error_null, [r, b], (num_r + num_b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11433422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1, Cov Error: 0.0017122572149002948, Cov Mag: 0.0011935513072305309\n",
      "i = 2, Cov Error: 0.0016705854516756802, Cov Mag: 0.0011935513072305309\n",
      "i = 3, Cov Error: 0.0016213574310335777, Cov Mag: 0.0011915176543595778\n",
      "i = 4, Cov Error: 0.0015654773420463704, Cov Mag: 0.0011915176543595778\n",
      "i = 5, Cov Error: 0.0015326943824483094, Cov Mag: 0.0011915176543595778\n",
      "i = 6, Cov Error: 0.001474680848593366, Cov Mag: 0.0011884507808995657\n",
      "i = 7, Cov Error: 0.001451157849326301, Cov Mag: 0.0011889302898977175\n",
      "i = 8, Cov Error: 0.001410961202641907, Cov Mag: 0.0011864094635682322\n",
      "i = 9, Cov Error: 0.0013218169479247222, Cov Mag: 0.0011900596504356609\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    print(f\"i = {i}, Cov Error: {cov_error(i/10.0, 0.9)[0]}, Cov Mag: {cov_error(i/10.0, 0.9)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = 950\n",
    "L = 900\n",
    "\n",
    "M = 1000.0\n",
    "Len = (U-L)\n",
    "Cov_Errors = [[cov_error(i/M, j/M) for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feacbe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1083529920168278\n",
      "(0.0012936687827024267, 0.0011785312830885496, 0.0008603427786633642, [0.9164, 0.9153], 104)\n"
     ]
    }
   ],
   "source": [
    "training_days = (startDate - endDate).days\n",
    "\n",
    "Sample_Size = [[Cov_Errors[i-L][j-L][4] for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "Abs_Errors = [[Cov_Errors[i-L][j-L][0]*(1+np.divide(1, Sample_Size[i-L][j-L]-1)) for i in range(L, U)] for j in range(L, U)]\n",
    "Cov_Mag = [[Cov_Errors[i-L][j-L][1] for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "Rel_Errors = [[np.divide(Abs_Errors[i-L][j-L], Cov_Mag[i-L][j-L]) for i in range(L, U)] for j in range(L, U)]\n",
    "\n",
    "min = np.argmin(Rel_Errors)\n",
    "\n",
    "print(Rel_Errors[min//Len][min%Len])\n",
    "print(Cov_Errors[min//Len][min%Len])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547512b",
   "metadata": {},
   "source": [
    "0.958157121980979\n",
    "(8.415090972179432e-06, 8.86868308873483e-06, [0.9152, 0.9153], 103)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orthogonal-alphas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
